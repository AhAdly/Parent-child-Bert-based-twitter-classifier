{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2efd5036-9124-4194-bb7e-020e582bbd4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44c93fe6-df64-4794-854e-a5e4416b56d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee190c82-4839-41a0-96dd-30b5a507ea37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a3b796d0-1d89-4f8a-9ae1-7125ab047ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d31830d-97f6-4584-aa53-97d59a6febfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 305954 entries, 0 to 305953\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   input1  305954 non-null  object\n",
      " 1   input2  305954 non-null  object\n",
      " 2   label   305954 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\").dropna().reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1fa59a9-121c-4bbd-b20e-4fb21622a4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    152977\n",
       "0    152977\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "025c7709-4fc0-4a89-8e68-a54a43200f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f7c8ea42-4642-4d4a-8515-22c3a5ba49f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 61191 entries, 192494 to 73867\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   input1  61191 non-null  object\n",
      " 1   input2  61191 non-null  object\n",
      " 2   label   61191 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test = df.sample(frac=0.2)\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "44d25681-8cb3-4399-a0e4-0bbbe0f018c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    30695\n",
       "0    30496\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "82d88720-ff9a-46b1-ad25-98c02f446781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(test.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87955114-0a16-4766-8ee2-6bce4dc6cb6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "num_labels = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a57fed34-5b95-4d15-8420-b08e0c8f5d34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "sentences = df[['input1', 'input2']].values\n",
    "labels = df['label'].values\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent[0],                 # Sentence to encode.\n",
    "                        sent[1],                 # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 240,          # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d90c5e1f-6589-430a-a087-c3530f9e0006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=44, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=44, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "766db861-da8d-4ed8-9f70-3332298f68c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create a TensorDataset.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "\n",
    "# Specify batch size.\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoader.\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              sampler = RandomSampler(train_data), \n",
    "                              batch_size = batch_size)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_data, \n",
    "                                   sampler = SequentialSampler(validation_data), \n",
    "                                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "879b6c06-048d-4e47-9ddd-9eba541c7e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Set the optimizer, learning rate, and number of training epochs.\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8\n",
    "                )\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "                                            \n",
    "# Set the device to GPU if available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8fbf0e5f-629e-46fd-b84e-8061049e1ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5d1852b-b8b9-489a-af36-2620f4e27686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a classification model.\n",
    "    \"\"\"\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    preds = np.argmax(preds, axis=1).flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Calculate accuracy using scikit-learn's accuracy_score function\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44457a00-733c-4649-baa5-faa0d8b396d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      0\n",
      "epoch      1\n",
      "epoch      2\n",
      "epoch      3\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch     \", epoch)\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    # Train the model on each batch of the training dataset.\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Clear out the gradients.\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "\n",
    "        # Get the loss value.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 to prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    # Evaluate the model on the validation dataset.\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this validation batch from our dataloader.\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother calculating gradients since we're only evaluating the model here.\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass (evaluate the model on this validation batch).\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "        # Get the loss value.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU memory and calculate the accuracy.\n",
    "        logits = outputs[1]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3f953657-88a5-4ab5-8efa-e01a746a8021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18369388758429622"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_eval_loss / len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a17ce963-e211-47cf-9c38-5885b345e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test[['input1', 'input2']].values\n",
    "test_labels = test['label'].values\n",
    "\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for sent in test_data:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent[0],                 # Sentence to encode.\n",
    "                        sent[1],                 # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 240,          # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "batch_size = 64\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "edb867ca-3a98-428b-8892-2164368dfa20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Define a function to calculate the flat F1 score\n",
    "def flat_f1(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return f1_score(labels, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0592bc3-f57e-4a44-a6be-7332d6d01348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_test_accuracy = 0\n",
    "total_test_loss = 0\n",
    "total_test_f1 = 0\n",
    "total_test_precision = 0\n",
    "total_test_recall = 0\n",
    "nb_test_steps = 0\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack this test batch from our dataloader.\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    # Tell pytorch not to bother calculating gradients since we're only evaluating the model here.\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass (evaluate the model on this test batch).\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "    # Get the loss value.\n",
    "    loss = outputs[0]\n",
    "\n",
    "    # Accumulate the test loss.\n",
    "    total_test_loss += loss.item()\n",
    "\n",
    "    # Move logits and labels to CPU memory and calculate the accuracy, F1 score, precision, and recall.\n",
    "    logits = outputs[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    total_test_accuracy += flat_accuracy(logits, label_ids)\n",
    "    total_test_f1 += flat_f1(logits, label_ids)\n",
    "    metrics = classification_report(label_ids, np.argmax(logits, axis=1), output_dict=True)\n",
    "    total_test_precision += metrics['weighted avg']['precision']\n",
    "    total_test_recall += metrics['weighted avg']['recall']\n",
    "    nb_test_steps += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "03583e4e-acfd-496b-ba8d-9d9365b7ed72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc058146-5856-44aa-8682-c71961dedb89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96a5b5ae-8314-4b09-8d77-8ab1a7595db8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.18\n",
      "Test Accuracy: 0.95\n",
      "Test F1 Score: 0.95\n",
      "Test Precision: 0.96\n",
      "Test Recall: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average test loss, accuracy, F1 score, precision, and recall.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "avg_test_f1 = total_test_f1 / len(test_dataloader)\n",
    "avg_test_precision = total_test_precision / len(test_dataloader)\n",
    "avg_test_recall = total_test_recall / len(test_dataloader)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Test Loss: {0:.2f}\".format(avg_test_loss))\n",
    "print(\"Test Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "print(\"Test F1 Score: {0:.2f}\".format(avg_test_f1))\n",
    "print(\"Test Precision: {0:.2f}\".format(avg_test_precision))\n",
    "print(\"Test Recall: {0:.2f}\".format(avg_test_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "934a92fb-db50-402e-b33d-7d832848ba4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5696c23a-9f11-4448-8958-1fbf0eb77509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fine_tuned_bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12dee5a2-880a-4662-a399-f25429b5daf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "28075a3c-439d-4957-a430-b903b12fe97a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61248"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "957 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "922404b4-dea4-46c6-8056-eb22a006dc27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61191"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f17577-d0ff-4e65-806e-ebf72016757d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
